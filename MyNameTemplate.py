import json
from typing import Annotated, Sequence
from openai.types.responses.response_reasoning_item import Summary
from typing_extensions import TypedDict
from dotenv import load_dotenv  
from langchain_core.messages import BaseMessage # The foundational class for all message types in LangGraph
from langchain_core.messages import ToolMessage # Passes data back to LLM after it calls a tool such as the content and the tool_call_id
from langchain_core.messages import SystemMessage # Message for providing instructions to the LLM
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langgraph.graph.message import add_messages
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from KIE_tools import *
# Explicitly import helper functions since 'from KIE_tools import *' skips underscores
from KIE_tools import _get_ppio_task_status_impl, _get_kie_task_status_impl 
from tool_prompts import SYSTEM_PROMPT
from pydantic import BaseModel, Field
from logger_util import get_logger


load_dotenv()
logger = get_logger("mynamechat.agent")


def log_system_message(message: str, echo: bool = False) -> None:
    """Helper to log a system-level message and optionally echo to console."""
    logger.info(message)
    if echo:
        print(message)

class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    last_task_id: str | None  # è®°å½•æœ€è¿‘ä¸€ä¸ªä»»åŠ¡çš„IDï¼Œç”¨äºç¼–è¾‘å›¾åƒæ—¶ï¼Œå¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šURLï¼Œä¸”æ²¡æœ‰æåˆ°retryï¼Œåˆ™ä½¿ç”¨æ­¤å€¼è¿›è¡ŒæŸ¥è¯¢
    last_tool_name: str | None # è®°å½•æœ€è¿‘ä¸€ä¸ªä»»åŠ¡ä½¿ç”¨çš„å·¥å…·åç§°ï¼Œç”¨äºåŒºåˆ†get_kie_task_statuså’Œget_ppio_task_status
    last_task_config: dict | None  # è®°å½•æœ€è¿‘ä¸€ä¸ªä»»åŠ¡çš„é…ç½®ï¼Œç”¨äºç¼–è¾‘å›¾åƒæ—¶ï¼Œå¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šURLï¼Œä¸”è¯´RETRYåˆ™ä½¿ç”¨æ­¤å€¼è¿›è¡Œé‡æ–°ç”Ÿæˆ
    global_config: dict | None  # è®°å½•å…¨å±€é…ç½®ï¼Œç”¨äºå‚¨å­˜æ¨¡æ¿çš„é…ç½®ï¼Œç”¨äºagentçš„èƒŒæ™¯çŸ¥è¯†å¡«å…¥APIè°ƒç”¨å‚æ•°
    references: list[dict] | None  # è®°å½•å‚è€ƒç´ æï¼Œæœ‰URLæ—¶è´Ÿè´£è®°å½•ï¼Œæ— URLæ—¶è´Ÿè´£æŒ‡ä»£å‚è€ƒç´ æ

class AgentResponse(BaseModel):
    answer: str = Field(description="The answer to the user's question")
    suggestions: list[str] = Field(description="The suggestions for the user to choose from")

tools = [
    # text_to_image_by_seedream_v4_model_create_task,
    image_edit_by_ppio_banana_pro_create_task,
    # get_task_status,
    text_to_video_by_kie_sora2_create_task,
    first_frame_to_video_by_kie_sora2_create_task,
    remove_watermark_from_image_by_kie_seedream_v4_create_task
    ]  # max function name length is 64

llm = ChatOpenAI(model = "gpt-5-nano",
                 temperature=0.0)

structured_llm = llm.with_structured_output(
    schema=AgentResponse,
    method="json_schema",
    strict=True,
    tools=tools,
    include_raw=True,
    reasoning_effort="medium"  # Can be "low", "medium", or "high"
    )


def recorder_node(state: AgentState) -> AgentState:
    """è®°å½•å™¨èŠ‚ç‚¹ï¼šä»å·¥å…·æ‰§è¡Œç»“æœä¸­æå–çŠ¶æ€å’Œæ›´æ–° References"""
    messages = state["messages"]
    new_state = {}
    
    # å€’åºéå†å¯»æ‰¾æœ€è¿‘çš„ AIMessage (è·å–å‚æ•°)
    last_ai_message = None
    for msg in reversed(messages):
        # æ£€æŸ¥æ˜¯å¦æ˜¯ AI æ¶ˆæ¯ä¸”æœ‰ tool_calls
        if msg.type == "ai" and hasattr(msg, "tool_calls") and msg.tool_calls:
            last_ai_message = msg
            break
            
    if not last_ai_message:
        logger.debug("Recorder: no AI message with tool_calls, skip")
        return {}

    # å»ºç«‹ ID åˆ°å‚æ•°çš„æ˜ å°„
    call_id_to_args = {call["id"]: call["args"] for call in last_ai_message.tool_calls}
    call_id_to_name = {call["id"]: call["name"] for call in last_ai_message.tool_calls}

    # å€’åºæŸ¥æ‰¾æœ€è¿‘çš„ ToolMessage
    for msg in reversed(messages):
        if msg.type == "tool":
            tool_call_id = msg.tool_call_id
            
            # åªå¤„ç†å±äºå½“å‰ AI æ¶ˆæ¯çš„ ToolMessage
            if tool_call_id in call_id_to_args:
                tool_name = call_id_to_name[tool_call_id]
                
                # 1. å¦‚æœæ˜¯ç”Ÿæˆç±»ä»»åŠ¡ -> è®°å½• ID, Config, ToolName
                if "create_task" in tool_name:
                    task_payload = msg.content
                    task_id = None
                    if isinstance(task_payload, dict):
                        task_id = task_payload.get("task_id") or task_payload.get("id")
                    elif isinstance(task_payload, str):
                        candidate = task_payload.strip()
                        if candidate.startswith("{") and candidate.endswith("}"):
                            try:
                                parsed = json.loads(candidate)
                                task_id = parsed.get("task_id") or parsed.get("id")
                            except json.JSONDecodeError:
                                task_id = candidate
                        else:
                            task_id = candidate
                    elif task_payload:
                        task_id = str(task_payload)

                    if not task_id:
                        logger.warning("Recorder: tool %s returned no task_id payload=%s", tool_name, task_payload)
                        continue

                    logger.info("Recorder captured task %s via tool %s", task_id, tool_name)
                    
                    new_state["last_task_id"] = task_id
                    new_state["last_tool_name"] = tool_name
                    new_state["last_task_config"] = call_id_to_args[tool_call_id]

                    break 
                        
    return new_state


def model_call(state:AgentState) -> AgentState:
    """æ¨¡å‹è°ƒç”¨èŠ‚ç‚¹ï¼šè´Ÿè´£æ„å»º Prompt å¹¶è°ƒç”¨ LLMï¼ŒåŒæ—¶å¤„ç†è‡ªåŠ¨åŠ è½½é€»è¾‘"""
    
    # --- è‡ªåŠ¨åŠ è½½ä¸Šä¸€è½®ç”Ÿæˆç»“æœ (Auto-Load Logic) ---
    # ä¿ç•™è‡ªåŠ¨æŸ¥è¯¢ï¼šå³ä½¿å‰ç«¯ä¹Ÿä¼šä¼ å› URLï¼Œæˆ‘ä»¬ä»æä¾›"æ— æ„ŸçŸ¥å…œåº•"ä½“éªŒï¼Œ
    # å°¤å…¶åœ¨ç”¨æˆ·è¿ç»­ç¼–è¾‘ã€æ²¡æœ‰é€‰æ‹© ref æ—¶ï¼Œå¯ä»¥è‡ªåŠ¨æŸ¥è¯¢ä¸Šä¸€è½®ä»»åŠ¡ç»“æœï¼Œå‡è½»äººå·¥æ“ä½œ
    
    current_refs = state.get("references", [])
    last_tid = state.get("last_task_id")
    last_tool = state.get("last_tool_name")
    
    # å¦‚æœå½“å‰æ²¡æœ‰å¼•ç”¨ï¼Œä¸”æœ‰ä¸Šä¸€è½®ä»»åŠ¡ï¼Œä¸”ä¸Šä¸€è½®æ˜¯å›¾åƒç¼–è¾‘ä»»åŠ¡ï¼Œå°è¯•è‡ªåŠ¨åŠ è½½
    if not current_refs:
        if last_tid and last_tool and "image_edit" in last_tool.lower():
            fetched_url = None
            log_system_message(f"[ç³»ç»Ÿ] å°è¯•è‡ªåŠ¨åŠ è½½ä¸Šä¸€è½®ä»»åŠ¡ç»“æœ (ID: {last_tid})...", echo=True)
            
            # æ ¹æ® Last Tool Name å†³å®šè°ƒç”¨å“ªä¸ªæŸ¥è¯¢å‡½æ•° (å¤ç”¨ KIE_tools å†…éƒ¨é€»è¾‘)
            if "ppio" in last_tool.lower() or "banana" in last_tool.lower():
                try:
                    res = _get_ppio_task_status_impl(last_tid)
                    if isinstance(res, str) and res.startswith("http"):
                        fetched_url = res
                except Exception as e:
                    log_system_message(f"[ç³»ç»Ÿ] PPIO æŸ¥è¯¢å¤±è´¥: {e}", echo=True)
            else:
                try:
                    res = _get_kie_task_status_impl(last_tid)
                    if isinstance(res, str) and res.startswith("http"):
                        fetched_url = res
                except Exception as e:
                     log_system_message(f"[ç³»ç»Ÿ] KIE æŸ¥è¯¢å¤±è´¥: {e}", echo=True)
            
            if fetched_url:
                log_system_message(f"[ç³»ç»Ÿ] âœ… æˆåŠŸåŠ è½½ä¸Šä¸€è½®ç»“æœ: {fetched_url}", echo=True)
                # ç›´æ¥æ›´æ–° stateï¼Œæœ¬è½®ç”Ÿæ•ˆï¼›å› ä¸ºä¸è¿”å›ï¼Œæ‰€ä»¥ä¸ä¼šæŒä¹…åŒ–åˆ°ä¸‹ä¸€è½®
                state["references"] = [{"url": fetched_url, "desc": "Last Generation Result (Auto-loaded)"}]
            else:
                log_system_message("[ç³»ç»Ÿ] â³ ä¸Šä¸€è½®ä»»åŠ¡ä»åœ¨å¤„ç†ä¸­æˆ–æ— æ³•è·å–ç»“æœã€‚", echo=True)
        else:
            logger.debug(
                "AutoLoad skipped: refs=%s last_tid=%s last_tool=%s",
                current_refs,
                last_tid,
                last_tool,
            )

    # 1. æ³¨å…¥åŠ¨æ€ä¸Šä¸‹æ–‡
    context_str = ""
    
    # æ³¨å…¥ç´ æåº“ (ä½¿ç”¨æœ¬è½®çš„ referencesï¼Œå¯èƒ½æ¥è‡ªç”¨æˆ·è¾“å…¥æˆ–è‡ªåŠ¨åŠ è½½)
    if state.get("references"):
        context_str += "\n### [REFERENCES]\n"
        for idx, asset in enumerate(state["references"]):
            context_str += f"{idx+1}. {asset.get('desc', 'Image')}: {asset.get('url')}\n"

    # æ³¨å…¥å…¨å±€é£æ ¼é…ç½®
    if state.get("global_config"):
        import json
        context_str += f"\n### [GLOBAL CONFIG]\n{json.dumps(state['global_config'], ensure_ascii=False)}\n"
        context_str += "INSTRUCTION: Always use these parameters (resolution, aspect_ratio, art_style, etc.) when calling tools unless the user explicitly overrides them in query.\n"

    if state.get("last_task_id"):
        context_str += f"\n[MEMORY] Last Task ID: {state['last_task_id']}"
    if state.get("last_task_config"):
        context_str += f"\n[MEMORY] Last Task Config: {state['last_task_config']}"
        
    # 2. ç»„åˆ Prompt
    system_prompt = SystemMessage(content=SYSTEM_PROMPT.format(tools_description=str(tools)) + context_str)
    
    # 3. è°ƒç”¨æ¨¡å‹
    response = structured_llm.invoke([system_prompt] + state["messages"])
    raw_response = response["raw"]
    
    # åªè¿”å› messagesï¼Œä¸è¿”å› references
    # references ä¼šåœ¨æœ¬è½®ä½¿ç”¨åï¼Œç”± recorder_node å¼ºåˆ¶æ¸…ç©ºï¼Œé¿å…æŒä¹…åŒ–åˆ°ä¸‹ä¸€è½®
    return {"messages": [raw_response]}


def should_continue(state: AgentState): 
    """åˆ¤æ–­æ˜¯å¦ç»§ç»­è°ƒç”¨å·¥å…·"""
    messages = state["messages"]
    last_message = messages[-1]
    if hasattr(last_message, 'tool_calls') and not last_message.tool_calls: 
        return "end"
    else:
        return "continue"
    

graph = StateGraph(AgentState)
graph.add_node("our_agent", model_call)


tool_node = ToolNode(tools=tools)
graph.add_node("tools", tool_node)
graph.add_node("recorder", recorder_node)

graph.set_entry_point("our_agent")

graph.add_conditional_edges(
    "our_agent",
    should_continue,
    {
        "continue": "tools",
        "end": END,
    },
)

graph.add_edge("tools", "recorder")
graph.add_edge("recorder", "our_agent")

app = graph.compile()

def print_stream(stream):
    for s in stream:
        message = s["messages"][-1]
        if isinstance(message, tuple):
            print(message)
        else:
            message.pretty_print()


async def chat_async():
    """æŒç»­å¯¹è¯æ¨¡å¼ - Tokençº§æµå¼è¾“å‡º"""
    from langchain_core.messages import AIMessage, HumanMessage
    
    # æ¬¢è¿ç•Œé¢
    print("\n" + "=" * 60)
    print("ğŸ¬  AI è§†é¢‘/å›¾åƒç”ŸæˆåŠ©æ‰‹ - ã€Šä½ çš„åå­—ã€‹ç»­é›†æ¨¡æ¿")
    print("=" * 60)
    
    # AI çš„å¼€åœºç™½
    greeting = ("ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„ AI åˆ›ä½œåŠ©æ‰‹ã€‚\n"
                "æˆ‘å¯ä»¥å¸®ä½ åŸºäºã€Šä½ çš„åå­—ã€‹åˆ›ä½œç»­é›†å†…å®¹ï¼š\n"
                "ğŸ“· æ ¹æ®è§’è‰²å‚è€ƒå›¾ç”Ÿæˆæ–°å›¾åƒ\n"
                "ğŸ¬ é€šè¿‡æ–‡æœ¬æˆ–é¦–å¸§ç”Ÿæˆè§†é¢‘\n"
                "è¾“å…¥ 'é€€å‡º' æˆ– 'exit' ç»“æŸå¯¹è¯ã€‚")
    
    # åˆå§‹åŒ– AgentState
    state: AgentState = {"messages": [AIMessage(content=greeting)]}
    print(f"\nAI: {greeting}\n")
    
    while True:
        user_input = input("ä½ : ")
        
        # é€€å‡ºæ£€æµ‹
        if user_input.lower() in ["exit", "quit", "é€€å‡º"]:
            print("ğŸ‘‹ å†è§ï¼")
            break
        if not user_input.strip():
            continue

        log_system_message(f"[INPUT] ç”¨æˆ·è¾“å…¥: {user_input[:100]}{'...' if len(user_input) > 100 else ''}", echo=True)
        # å°è¯•è§£æ JSON è¾“å…¥
        try:
            input_data = json.loads(user_input)
            query = input_data.get("user_query", "")
            # æ˜¾å¼è¦†ç›– references
            incoming_refs = input_data.get("references", [])
            state["references"] = incoming_refs
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯ (åªåŒ…å« query)
            state["messages"].append(HumanMessage(content=query))
            
            # æ—¥å¿—è®°å½• JSON è§£æç»“æœ
            log_system_message(f"[INPUT] JSON è§£ææˆåŠŸ - query: {query[:50]}{'...' if len(query) > 50 else ''}", echo=True)
            log_system_message(f"[INPUT] references æ•°é‡: {len(incoming_refs)}", echo=True)
            if incoming_refs:
                for i, ref in enumerate(incoming_refs):
                    log_system_message(f"[INPUT]   [{i+1}] url: {ref.get('url', 'N/A')[:80]}", echo=True)
            else:
                log_system_message("[INPUT]   (ç©ºåˆ—è¡¨)", echo=True)
            log_system_message(f"[INPUT] last_task_id: {state.get('last_task_id', 'None')}", echo=True)
            log_system_message(f"[INPUT] last_tool_name: {state.get('last_tool_name', 'None')}", echo=True)
            
        except json.JSONDecodeError:
            # æ™®é€šæ–‡æœ¬è¾“å…¥ -> æ¸…ç©ºä¸Šä¸€è½®çš„å‚è€ƒç´ æ
            state["references"] = []
            
            # æ—¥å¿—è®°å½•çº¯æ–‡æœ¬è§£æ
            log_system_message("[INPUT] çº¯æ–‡æœ¬è¾“å…¥ (é JSON)", echo=True)
            log_system_message("[INPUT] references: [] (å·²æ¸…ç©º)", echo=True)
            log_system_message(f"[INPUT] last_task_id: {state.get('last_task_id', 'None')}", echo=True)
            log_system_message(f"[INPUT] last_tool_name: {state.get('last_tool_name', 'None')}", echo=True)
            # Config å¯ä»¥é€‰æ‹©ä¿ç•™ï¼ˆå› ä¸ºå®ƒé€šå¸¸æ˜¯å…¨å±€çš„ï¼‰ï¼Œæˆ–è€…ä¹Ÿæ¸…ç©ºï¼Ÿ
            # æ ¹æ®ä½ çš„éœ€æ±‚"æ¯æ¬¡éƒ½æ ¹æ®ç”¨æˆ·çš„æ–°æé—®æ¥å†³å®š"ï¼Œè¿™é‡Œæœ€å¥½ä¹Ÿé‡ç½®ï¼Œæˆ–è€…æ˜¯ä¿æŒé»˜è®¤ã€‚
            # ä½†é€šå¸¸ Config (é£æ ¼) æ˜¯ç›¸å¯¹ç¨³å®šçš„ï¼ŒAssets (ç´ æ) æ˜¯æ˜“å˜çš„ã€‚
            # ä¸ºäº†å®‰å…¨èµ·è§ï¼Œä¸”ç¬¦åˆ"æ— çŠ¶æ€"è®¾è®¡ï¼Œæˆ‘ä»¬è¿™é‡Œé€‰æ‹©ä¸ä¸»åŠ¨æ¸…ç©º Config (å‡è®¾ç”¨æˆ·æ²¡ä¼ å°±æ˜¯æ²¿ç”¨æ—§çš„æˆ–è€…é»˜è®¤)ï¼Œ
            # ä½† Assets å¿…é¡»æ¸…ç©ºã€‚
            
            state["messages"].append(HumanMessage(content=user_input))
        
        # ä½¿ç”¨ Token çº§æµå¼è¾“å‡º
        print()
        
        # è·Ÿè¸ªçŠ¶æ€
        in_agent_response = False
        shown_ai_prefix = False
        
        # ä½¿ç”¨ astream_events å®ç° Token çº§æµå¼ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
        async for event in app.astream_events(state, version="v2"):
            kind = event["event"]
            
            # æ•è· LLM çš„æµå¼ token
            if kind == "on_chat_model_stream":
                content = event["data"]["chunk"].content
                if content:
                    if not shown_ai_prefix:
                        print("AI: ", end="", flush=True)
                        shown_ai_prefix = True
                    print(content, end="", flush=True)
                    in_agent_response = True
            
            # æ•è·å·¥å…·è°ƒç”¨ä¿¡æ¯
            elif kind == "on_tool_start":
                tool_name = event["name"]
                if in_agent_response:
                    print()  # æ¢è¡Œ
                    in_agent_response = False
                print(f"\n[ğŸ”§ è°ƒç”¨å·¥å…·: {tool_name}]", flush=True)
                shown_ai_prefix = False  # é‡ç½®ï¼Œä¸‹æ¬¡æ¨¡å‹è¾“å‡ºæ—¶å†æ˜¾ç¤º
            
            elif kind == "on_tool_end":
                print(f"[âœ“ å·¥å…·æ‰§è¡Œå®Œæˆ]\n", flush=True)
                in_agent_response = False
            
            # æ•è·æœ€ç»ˆçŠ¶æ€æ›´æ–°
            elif kind == "on_chain_end" and event.get("name") == "LangGraph":
                # è·å–æœ€ç»ˆè¾“å‡ºçŠ¶æ€
                state = event["data"]["output"]
        
        print("\n")  # ç©ºè¡Œåˆ†éš”


def chat():
    """åŒæ­¥åŒ…è£…å™¨ - è°ƒç”¨å¼‚æ­¥ chat å‡½æ•°"""
    import asyncio
    
    try:
        asyncio.run(chat_async())
    except KeyboardInterrupt:
        print("\n\nç¨‹åºå·²ä¸­æ–­ã€‚å†è§ï¼")


if __name__ == "__main__":
    chat()
