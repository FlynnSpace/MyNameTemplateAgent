from typing import Annotated, Sequence
from openai.types.responses.response_reasoning_item import Summary
from typing_extensions import TypedDict
from dotenv import load_dotenv  
from langchain_core.messages import BaseMessage # The foundational class for all message types in LangGraph
from langchain_core.messages import ToolMessage # Passes data back to LLM after it calls a tool such as the content and the tool_call_id
from langchain_core.messages import SystemMessage # Message for providing instructions to the LLM
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langgraph.graph.message import add_messages
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from KIE_tools import *
from tool_prompts import SYSTEM_PROMPT
from pydantic import BaseModel, Field


load_dotenv()

class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]

class AgentResponse(BaseModel):
    answer: str = Field(description="The answer to the user's question")
    suggestions: list[str] = Field(description="The suggestions for the user to choose from")

tools = [
    # text_to_image_by_seedream_v4_model_create_task,
    image_edit_by_seedream_v4_edit_create_task,
    get_task_status,
    text_to_video_by_sora2_model_create_task,
    first_frame_to_video_by_sora2_model_create_task,
    remove_watermark_from_image_by_seedream_v4_edit_create_task
    ]  # max function name length is 64

model = ChatOpenAI(model = "gpt-5-nano",
                 model_kwargs={"response_format": AgentResponse},
                 reasoning_effort="low"   # Can be "low", "medium", or "high"
                 ).bind_tools(tools, strict=True)   # Can be "auto", "concise", or detailed"

def model_call(state:AgentState) -> AgentState:
    system_prompt = SystemMessage(content=SYSTEM_PROMPT.format(tools_description=str(tools)))
    response = model.invoke([system_prompt] + state["messages"])
    return {"messages": [response]}


def should_continue(state: AgentState): 
    """åˆ¤æ–­æ˜¯å¦ç»§ç»­è°ƒç”¨å·¥å…·"""
    messages = state["messages"]
    last_message = messages[-1]
    if hasattr(last_message, 'tool_calls') and not last_message.tool_calls: 
        return "end"
    else:
        return "continue"
    

graph = StateGraph(AgentState)
graph.add_node("our_agent", model_call)


tool_node = ToolNode(tools=tools)
graph.add_node("tools", tool_node)

graph.set_entry_point("our_agent")

graph.add_conditional_edges(
    "our_agent",
    should_continue,
    {
        "continue": "tools",
        "end": END,
    },
)

graph.add_edge("tools", "our_agent")

app = graph.compile()

def print_stream(stream):
    for s in stream:
        message = s["messages"][-1]
        if isinstance(message, tuple):
            print(message)
        else:
            message.pretty_print()


async def chat_async():
    """æŒç»­å¯¹è¯æ¨¡å¼ - Tokençº§æµå¼è¾“å‡º"""
    from langchain_core.messages import AIMessage, HumanMessage
    
    # æ¬¢è¿ç•Œé¢
    print("\n" + "=" * 60)
    print("ğŸ¬  AI è§†é¢‘/å›¾åƒç”ŸæˆåŠ©æ‰‹ - ã€Šä½ çš„åå­—ã€‹ç»­é›†æ¨¡æ¿")
    print("=" * 60)
    
    # AI çš„å¼€åœºç™½
    greeting = ("ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„ AI åˆ›ä½œåŠ©æ‰‹ã€‚\n"
                "æˆ‘å¯ä»¥å¸®ä½ åŸºäºã€Šä½ çš„åå­—ã€‹åˆ›ä½œç»­é›†å†…å®¹ï¼š\n"
                "ğŸ“· æ ¹æ®è§’è‰²å‚è€ƒå›¾ç”Ÿæˆæ–°å›¾åƒ\n"
                "ğŸ¬ é€šè¿‡æ–‡æœ¬æˆ–é¦–å¸§ç”Ÿæˆè§†é¢‘\n"
                "è¾“å…¥ 'é€€å‡º' æˆ– 'exit' ç»“æŸå¯¹è¯ã€‚")
    
    # åˆå§‹åŒ– AgentState
    state: AgentState = {"messages": [AIMessage(content=greeting)]}
    print(f"\nAI: {greeting}\n")
    
    while True:
        user_input = input("ä½ : ")
        
        # æ£€æŸ¥é€€å‡ºå‘½ä»¤
        if user_input.lower().strip() in ["é€€å‡º", "exit", "quit", "ç»“æŸ", "å†è§"]:
            print("\nAI: å†è§ï¼æœŸå¾…ä¸‹æ¬¡ä¸ºä½ åˆ›ä½œç²¾å½©å†…å®¹ã€‚ğŸ‘‹\n")
            break
        
        # æ£€æŸ¥ç©ºè¾“å…¥
        if not user_input.strip():
            print("AI: è¯·è¾“å…¥æœ‰æ•ˆçš„å†…å®¹ã€‚\n")
            continue
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ° state
        state["messages"].append(HumanMessage(content=user_input))
        
        # ä½¿ç”¨ Token çº§æµå¼è¾“å‡º
        print()
        
        # è·Ÿè¸ªçŠ¶æ€
        in_agent_response = False
        shown_ai_prefix = False
        
        # ä½¿ç”¨ astream_events å®ç° Token çº§æµå¼ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
        async for event in app.astream_events(state, version="v2"):
            kind = event["event"]
            
            # æ•è· LLM çš„æµå¼ token
            if kind == "on_chat_model_stream":
                content = event["data"]["chunk"].content
                if content:
                    if not shown_ai_prefix:
                        print("AI: ", end="", flush=True)
                        shown_ai_prefix = True
                    print(content, end="", flush=True)
                    in_agent_response = True
            
            # æ•è·å·¥å…·è°ƒç”¨ä¿¡æ¯
            elif kind == "on_tool_start":
                tool_name = event["name"]
                if in_agent_response:
                    print()  # æ¢è¡Œ
                    in_agent_response = False
                print(f"\n[ğŸ”§ è°ƒç”¨å·¥å…·: {tool_name}]", flush=True)
                shown_ai_prefix = False  # é‡ç½®ï¼Œä¸‹æ¬¡æ¨¡å‹è¾“å‡ºæ—¶å†æ˜¾ç¤º
            
            elif kind == "on_tool_end":
                print(f"[âœ“ å·¥å…·æ‰§è¡Œå®Œæˆ]\n", flush=True)
                in_agent_response = False
            
            # æ•è·æœ€ç»ˆçŠ¶æ€æ›´æ–°
            elif kind == "on_chain_end" and event.get("name") == "LangGraph":
                # è·å–æœ€ç»ˆè¾“å‡ºçŠ¶æ€
                state = event["data"]["output"]
        
        print("\n")  # ç©ºè¡Œåˆ†éš”


def chat():
    """åŒæ­¥åŒ…è£…å™¨ - è°ƒç”¨å¼‚æ­¥ chat å‡½æ•°"""
    import asyncio
    
    try:
        asyncio.run(chat_async())
    except KeyboardInterrupt:
        print("\n\nç¨‹åºå·²ä¸­æ–­ã€‚å†è§ï¼")


if __name__ == "__main__":
    chat()
